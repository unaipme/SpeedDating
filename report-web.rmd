---
title: "Speed Dating"
knit: (function(input_file, encoding) { rmarkdown::render(input_file, encoding=encoding, output_file=file.path(dirname(input_file), "docs", 'index.html')) })
author: "Mehmet Fatih Cagil & Unai Perez"
header-includes:
  - \usepackage{xcolor}
  - \usepackage{float}
  - \renewcommand{\baselinestretch}{1.15}
output:
  rmdformats::readthedown:
    self_contained: true
---

```{r include=F}
set.seed(666)
library(stringdist)
library(mice)
library(chemometrics)
library(nnet)
library(caret)
library(FactoMineR)
library(shape)
library(e1071)
library(fpc)
library(cclust)
library(RColorBrewer)

norm.score <- function(row) {
  m <- max(row, na.rm = T)
  s <- sum(row, na.rm=T)
  lapply(row, function(x) 10 * ifelse(is.na(x), 0, x) / (m * (1 + (100 - s) / 100)))
}

speed.dating <- read.csv("speeddating.csv", na.strings = c('', '?'))
```

# Introduction

The _Speed Dating_ dataset is the result of a experiment run by Fisman et al. from 2002 to 2004 The experiment consisted in engaging participant subjects, drawn from students in graduate and professional schools at Columbia University. These subjects would share conversations for four minutes and then they would decide if they both accepted in meeting each other again. Before the dating event, participants would fill in a form giving themselves a rating for **attractiveness**, **sincerity**, **intelligence**, ability to be **funny** and **ambition**. They also had to rate their interests in different topics. After the dating event, the participants would need to rate their partners' on the same characteristics for which they rated themselves. The dating event was organized in waves, and the participants randomly distributed among them, so that subjects in one wave would not interact with participants in other waves.

Below, a detailed explanation of the variables. The names of the variables are not too self-explanatory. Whenever the notation "`d_`" is used, it refers to a categorically discretized version of a numerical variable that is also in the set. Whenever "`_o`" is used, it has the same meaning as without it, but for subject 2 instead of 1.

- `has_null`: Binary feature that is `1` if there is any missing value in the row.
- `wave`: The number of the wave in which the pairing took place.
- `gender`: Categorical feature specifying the gender of subject 1.
- `age[_o]`: Age of subjects 1 and 2, respectively.
- `[d_]d_age`: Features that represent the age difference of the subjects.
- `race[_o]`: Categorical features expressing the race of subjects 1 and 2, respectively.
- `samerace`: Binary feature that is `1` if both subjects are of the same race.
- `[d_]importance_same_{race|religion}`: A score from 0 to 10 expressing how important it is that the other subject shares race and religion, respectively.
- `field`: Categorical variable describing the field of the studies of the subjects.
- `[d_]pref_o_{attractive|sincere|intelligence|funny|ambitious|shared_interests}`: A score from 0 to 100 representing the preference of the subject 2 regarding each one of the characteristics mentioned earlier in their partner.
- `[d_]{attractive|sincere|intelligence|funny|ambitious|shared_interests}_o`: A score from 0 to 10 representing the score subject 2 gives to subject 1 for each of the characteristics.
- `[d_]{attractive|sincere|intelligence|funny|ambitious|shared_interests}_important`: A score from 0 to 100 representing how important is for the subject 1 each of the characteristics in their partner.
- `[d_]{sports|tvsports|exercise|dining|museums|art|hiking|gaming|clubbing|reading|tv |theater|movies|concerts|music|shopping|yoga}`: A score from 0 to 10 representing the interest the subject 1 has in each of the topics.
- `[d_]interests_correlate`: The correlation, measured in the range [-1,1], of the interests of the two subjects.
- `[d_]expected_happy_with_sd_people`: A score from 0 to 10 of how satisfied the subject 1 expects to be with the people met in the speed dating.
- `[d_]expected_num_interested_in_me`: Out of the 20 people subject 1 will meet, the amount they expect to be interested in them after the date.
- `[d_]expected_num_matches`: The amount of matches the subject 1 expects to get.
- `[d_]like`: A binary feature that is `1` if the subject 1 liked its date.
- `[d_]guess_prob_liked`: A probability, guessed by subject 1, of whether they think the subject 2 liked the date.
- `met`: A binary feature that is `1` if the subject 1 new subject 2 beforehand.
- `decision[_o]`: Binary features that are `1` if the subjects decide they want to see the other subject in the future.
- `match`: Target binary feature. `1` if both subjects agree on seeing each other in the future.

The features that are scored out of 100 work differently in comparison to regular grades. The subjects were given a 100 points to distribute among the five criteria and the `shared_interests` criteria. This means that the sum of the scores in these blocks should add up to 100.

# Data preprocessing

We know data pre-processing is long and tough (let's not make any joke about this), and this dataset was not easy to work with from the get-go. Here is how we preprocessed the data.

## Feature name typos

One weird characteristic about this dataset is the lack of consistency and the bad ortography in the names of the features. In the list above, all the names have been correctly written. However, "sincere" is many times misspelled as "sinsere", or "ambitious" as "ambitous". Also, sometimes "ambition" is used instead. The first step is to just fix the names of the variables.

```{r include=F}
names(speed.dating)[which(names(speed.dating) %in% c("sinsere_o", "ambitous_o", "d_sinsere_o", "d_ambitous_o", "ambtition_important", "d_ambtition_important"))] <- 
  c("sincere_o", "ambitious_o", "d_sincere_o", "d_ambitious_o", "ambition_important", "d_ambition_important")
```

## Missing values

A first look at the dataset shows that missing data is encoded as question marks (`?`). These can be transformed into R's `NA` as soon as they are read, and can be dealt with later on. In total, `r length(which(sapply(speed.dating, anyNA)))` features have at least one missing value. If feature `has_null` is correctly labelled, then the amount of observations with `has_null` set to 1 (which equals to `r length(which(speed.dating$has_null == 1))`) and the amount of observations with a `NA` in any of the columns (which equals to `r nrow(speed.dating) - length(which(complete.cases(speed.dating)))`) should be, and are, the same. A huge majority (`r round(100 * nrow(speed.dating[speed.dating$has_null == 1,]) / nrow(speed.dating), 2)`%) of the observations in the dataset have some missing value, just deleting them is not an option.

The summary reveals a series of potentially meaningful patterns when it comes to missing values. Features can be grouped as shown in the list of the previous section, into what would translate as (let's call it) a block of questions from the form. To begin with, all of the features from the block of interests (like `sports` or `hiking`) have the same amount of missing values, `r length(which(is.na(speed.dating$sports)))`, all of them for the same individuals. The block that represents the factors of the partner that are most important for the subject, like `attractive_important`, show a similar behaviour. That is also the case for the self ratings in the five criteria, like `attractive` and `intelligence`. They have at least the same amount of missing values, and most of do coincide in the same observations. It seems like a lot of people refused to fill the form before the dates! These people will be of no use for our research, so those rows will be deleted.

```{r include=F}
speed.dating <- speed.dating[!(is.na(speed.dating$sports)),]
```

Looking at the summary after the deletion of said rows, two more patterns can be seen. Self ratings, like `intelligence`, seem to be unknown more the same group of observations, exactly `r nrow(speed.dating[is.na(speed.dating$intelligence),])`. Meanwhile, the partner's preference features, like `pref_o_sincere`, share a very similar amount of missing values. Although the missing values of these two groups do not coincide in the same observations, it is interesting to see that specific subgroups of variables have missing values for the same individuals. These two groups are removed.

```{r include=F}
speed.dating <- speed.dating[!(is.na(speed.dating$pref_o_sincere)),]
speed.dating <- speed.dating[!(is.na(speed.dating$attractive)),]
```

There seems to be a couple more of patterns. Both age related features are missing in the same observations, but the `d_age` feature describing age difference is there, so we could consider that data is not missing. Oddly enough, these missing observations belong to couples with more than 18 years of difference in age. The last pattern seems to relate the missing values of the ratings given by the partners (`attractive_o` and so on) and the ratings given by the subjects to the partner (`attractive_partner` and so on). But there is no actual connection.

After those patterns, all of the participants that did not respond at all to, at least, one of the blocks will be deleted. This is only the case for two blocks. The first one is the rating that subject 2 gives subject 1 regarding the five criteria. The second block is the opposite, the ratings subject 1 give to subject 2 regarding those criteria. The features that represent categorically the same meaning (those named the same but starting with "`d_`") do not seem to hold any value for these individuals anyway; they are all assigned, seemingly by default, to the category `[0-5]`. All these observations are also deleted.

```{r include=F}
speed.dating <- speed.dating[-which(apply(is.na(speed.dating[, 28:33]), 1, all)),]
speed.dating <- speed.dating[-which(apply(is.na(speed.dating[, 62:67]), 1, all)),]
```

Finally, we are going to check those features that represent scores out of 10, this is, those blocks made up of the variables named `_partner` and `_o`. Some of these have many missing values. The one with the most NAs is `shared_interests_partner`, which has `r length(which(is.na(speed.dating$shared_interests_partner)))`. If we look at the values of the equivalent categorical features, all of these NA are assigned to the group `[0-5]`. The missing value could be caused by the subject just not caring too much about that one criterion in particular. So, we have decided to assign `0` to these missing values. The same thing can be applied to the missing values of the features with scores out of 100 (`pref_o` and `_important` blocks).

```{r include=F}
for (i in c(16:21, 28:33, 40:45, 62:67)) {
  speed.dating[is.na(speed.dating[,i]), i] <- 0
}
```

## Variable typing and coherence

The summary shows that some grades are over the range. For example, one of the participants graded herself as an 11 out of 10 in `funny`, which is funny by itself because the subject is studying the _law_ field, and we don't know any funny lawyer. For feature `gaming`, there are 5 different participants that graded their interests as 14 out of 10. These definitely need to be fixed. There are 38 variables that should range from 0 to 10, from which 4 have some value out of those bounds. For the `gaming` example, categorical feature `d_gaming` groups the incoherent values into `[9-10]`. This makes us believe that all grades over 10 should be turned back into just 10.

```{r include=F}
speed.dating$gaming[!(is.na(speed.dating$gaming)) & speed.dating$gaming > 10] <- 10
speed.dating$funny_o[!(is.na(speed.dating$funny_o)) & speed.dating$funny_o > 10] <- 10
speed.dating$attractive_o[!(is.na(speed.dating$attractive_o)) & speed.dating$attractive_o > 10] <- 10
speed.dating$reading[!(is.na(speed.dating$reading)) & speed.dating$reading > 10] <- 10
```

As explained in the introduction, there is a block of features that represent the preferences of the subjects by distributing 100 points among them. This is not always the case, though. Some participants did not actually completely use the 100 points, but this is addressed later in the document. The problem comes from those that used more than 100 points. These guys clearly do not know how to add up. For the sake of consistency, we have decided to remove these observations. Before, we have checked their field of studies just out of curiosity, and many of these are mathematicians and engineers. Future is bright!

```{r include=F}
speed.dating <- speed.dating[-c(which(floor(apply(speed.dating[,16:21], 1, sum)) > 100), which(floor(apply(speed.dating[,40:45], 1, sum)) > 100)),]
```

Some of those subjects, though, just exceed the 100 points by decimals. To avoid having problems in the future, these decimals have been substracted from them so that their score will add up to exactly 100. 

```{r include=F}
speed.dating[which(apply(speed.dating[,16:21], 1, sum) > 100),16:21] <- speed.dating[which(apply(speed.dating[,16:21], 1, sum) > 100),16:21] - ((apply(speed.dating[which(apply(speed.dating[,16:21], 1, sum) > 100),16:21], 1, sum) - 100) / 6)
speed.dating[which(apply(speed.dating[,40:45], 1, sum) > 100),40:45] <- speed.dating[which(apply(speed.dating[,40:45], 1, sum) > 100),40:45] - ((apply(speed.dating[which(apply(speed.dating[,40:45], 1, sum) > 100),40:45], 1, sum) - 100) / 6)
```

Finally, seeing that some of the types automatically assigned to the features by R when reading the file are wrong, we decided to change them. Also, we have discussed whether the score features should be considered factors but, due to one process run for feature extraction, numbers with decimals are obtained. Some come already with decimals from the file, too. We have decided that it would obviously cause innaccuracy, so scores will be considered as numeric values. We already have categorical values out of the box for scores anyway.

```{r include=F}
speed.dating$has_null <- as.factor(speed.dating$has_null)
speed.dating$wave <- as.factor(speed.dating$wave)
speed.dating$samerace <- as.factor(speed.dating$samerace)
speed.dating$decision <- as.factor(speed.dating$decision)
speed.dating$decision_o <- as.factor(speed.dating$decision_o)
speed.dating$match <- as.factor(speed.dating$match)
```

## Feature selection and extraction

The block of features representing the preferences of the subjects regarding the five criteria (`_important` block for subject 1 and `pref_o_` block for subject 2) are scores out of 100. The rest of scores are out of 10. But, as explained in the introduction, these features work differently. If the out of 100 scores were regular scores, it would be better to multiply by 10 the out of 10 scores to avoid inaccuracies. In this case, we have decided to use the following formula to turn each feature in its proportional score between 0 and 10, based on the criteria with the highest score:

$$ score^{(i)}_{10} = \frac{10 \times score^{(i)}_{100}}{\max(score_{100}) \times \lambda} $$

Where $\lambda$ represents a penalization that has been added to the denominator because some subjects did not distribute the available 100 points (as explained in the introduction) completely. $\lambda$ is calculated as shown below:

$$ \lambda = 1 + \frac{100 - \sum_{i'} score^{(i')}_{100}}{100} $$

```{r include=F}
for (i in 1:nrow(speed.dating)) {
  speed.dating[i,16:21] <- norm.score(speed.dating[i, 16:21])
  speed.dating[i,40:45] <- norm.score(speed.dating[i, 40:45])
}
```

This transformation would need to be stored in completely different variables, but we have decided to do without the original out of 100 scores. Thus, they have been overwritten. That has been the case too for the equivalent categorical variables (the ones with same name but starting with `d_`), which have been replaced using the same categories as the rest of the categorical variables: `[0-5]`, `[6-8]` and `[9-10]`. We think these represent the different feelings a person can have towards something appropriately; they could be translated as "dislike", "like", and "love".

```{r include=F}
for (i in c(16:21, 40:45)) {
  speed.dating[,i+6] <- cut(speed.dating[,i], breaks=c(0,5,8,10), include.lowest = T, labels=c("[0-5]","[6-8]","[9-10]"))
}
```

Feature `field` brings interesting information to the table. With this information, we could find correlation between the field of study of the subjects and, maybe, the probability of this being influential in the final choice. Nevertheless, the actual categories of the feature are filled with typos, differently formatted but equivalent answers, and globally or specifically ambiguous answers. For example, `finance` and `finanace` are two different categories, as well as `finance/economics` and `finance&economics`, and, finally, there are categories just labelled as `money` or the different categories `english` and `english education`, whose differences we are not really able to tell.

We tried performing some sort of entity resolution by transforming categories to lower case, then applying Levenshtein edit distance to find similarities. A big maximum distance (like 5) would match `english` from `polish`, an unwanted result. A small maximum distance (like 2) would detect most typos but leave out categories with larger differences. It all came back to manual selection, which for `r length(levels(speed.dating$field))` different categories is cumbersome and tiring. We decided to get rid of the feature completely.

```{r include=F}
speed.dating$field <- NULL
```

Now that we are discarding features, we decide to discard the features `age`, `age_o` and `d_age`. We do not think the actual ages are all that important, or event the age difference per se; variable `d_d_age`, categorical representation of the age difference, will do just fine. Also, feature `expected_num_interested_in_me` will be discarded, due to its `r length(which(is.na(speed.dating$expected_num_interested_in_me)))` missing variables. Features `importance_same_religion` and `d_importance_same_religion` get discarded too because we do not actually have the religion data, so there is nothing to compare it to (They must have slipped up!). And we do not need `has_null` anymore. The rest of features, a total of `r ncol(speed.dating) - 10`, are staying. We think that having different representations of the same data directly from the raw file saves us time and effort, and opens up the possibility to apply different models without more extracted features.

```{r include=F}
speed.dating$age <- NULL
speed.dating$age_o <- NULL
speed.dating$d_age <- NULL
speed.dating$expected_num_interested_in_me <- NULL
speed.dating$met <- NULL
speed.dating$decision <- NULL
speed.dating$decision_o <- NULL
speed.dating$importance_same_religion <- NULL
speed.dating$d_importance_same_religion <- NULL
speed.dating$has_null <- NULL
```

## Data imputation

After the all the preprocessing so far, only `r length(which(sapply(speed.dating, anyNA)))` features with NAs are left. These have been imputed using the MICE imputation method, using the five criteria parameters and the `_important` block as the only supplementary variables. Using the whole dataset slows down the whole process and makes it last up to 30 times more. Using only these features still gives realistic imputation, so we considered it appropriate.

```{r include=F}
mice.speed.dating <- mice(speed.dating[,c(which(sapply(speed.dating, anyNA)), 37:42, 47:52)], method = 'pmm', seed = 500)
mice.speed.dating.complete <- complete(mice.speed.dating)
speed.dating[, which(sapply(speed.dating, anyNA))] <- mice.speed.dating.complete[,1:length(which(sapply(speed.dating, anyNA)))]
```

## Outlier detection

The dataset has `r ncol(speed.dating)` left after the preprocessing. They are still plenty, so trying to find univariate outliers is not only annoying, but also useless. For the multivariate outlier detection, the Mahalanobis distance has been used. The figure below shows a graphical representation of the distances of each observation, where the red dots represent the observations with a match, and the blue ones observations without a match. On plain sight, there does not seem to be any correlation between the outcome and the distance (or the _outlierness_, we could say). If anything, we can state that the few that stand out the most are non-matches. The 2.5% of the observations with the highest Mahalanobis distance, shown in the figure above the line, will be considered outliers and removed.

<center>
```{r multivar-outliers, echo=F, fig.cap="\\label{fig:multivar-outliers}Graphical representation of the Mahalanobis distances for each observation", fig.align="center"}
dating.outliers <- Moutlier(scale(speed.dating[,sapply(speed.dating, is.numeric)]), plot=F, quantile = 0.975)
par(mfrow=c(1,2))
plot(dating.outliers$md, main="Mahalanobis Distance", xlab="Distance", ylab="Index", col=ifelse(speed.dating$match == "1", "orangered1", "royalblue1"), las=1, cex.axis=.75, pch=20, ylim=c(0, max(dating.outliers$rd)), xaxt="n")
plot(dating.outliers$rd, main="Robustified Mahalanobis", xlab="Distance", ylab="Index", col=ifelse(speed.dating$match == "1", "orangered1", "royalblue1"), las=1, cex.axis=.75, pch=20, ylim=c(0, max(dating.outliers$rd)), xaxt="n")
cutoff <- sort(dating.outliers$rd, decreasing = T)[floor(nrow(speed.dating) * 0.025)]
abline(h = cutoff, lty=2, col="red4", lwd=2.25)
par(mfrow=c(1,1))
```
</center>

```{r include=F}
speed.dating <- speed.dating[-order(dating.outliers$rd, decreasing = T)[1:floor(nrow(speed.dating) * 0.025)],]
```

## Data visualization

PCA allows us to see a representation of the dataset in two dimension of the data. With this, we will perform a few clustering methods and see if we can extract something useful out of it. PCA can only be used with numeric explanatory variables, so the variable `match` was added as qualitative suplementary variable. This means a total of `r length(which(sapply(speed.dating, is.numeric))) + 1` variables. The figure below shows the representation in the first factorial plane of _only_ the 10 variables with the most representation and, still, they are not really well represented, probably due to the high dimensionality of the data.

```{r include=F}
speed.dating.pca <- speed.dating[,c(which(sapply(speed.dating, is.numeric)), 112)]
for (i in 1:52) {
  speed.dating.pca[,i] <- speed.dating.pca[,i] - mean(speed.dating.pca[,i])
}
pca.output <- PCA(speed.dating.pca, quali.sup = c(53), graph = F)
pca.best.repr.vars <- order(sqrt(pca.output$var$coord[,1]^2 + pca.output$var$coord[,2] ^ 2), decreasing = T)[1:10]
```

<center>
```{r echo=F, fig.cap="\\label{fig:pca-var-repr}Variable representation in the first factorial plane", fig.pos="H", out.extra="", fig.align="center", fig.height=4.25}
plot(pca.output$var$coord, col=0, xlim=c(-2.5,1.5), ylim=c(-.8,.8), las=1, cex.axis=.7, cex.lab=.8, xlab="", ylab="")
abline(h=0, col="gray")
abline(v=0, col="gray")
plotellipse(1, 1)
invisible(Arrows(0, 0, pca.output$var$coord[pca.best.repr.vars,1], pca.output$var$coord[pca.best.repr.vars,2], arr.type = "simple", col=rainbow(10)))
legend("topleft", legend=row.names(pca.output$var$coord[pca.best.repr.vars,]), col=rainbow(10), lty=1, cex=.7, bg="white")
```
</center>

In the figure below, the representation of the individuals and the categories of the `match` supplementary variable can be seen. None of those seems to be too tied to any of the dimensions.

<center>
```{r echo=F, fig.cap="\\label{fig:pca-ind-repr}Representation of individuals in first factorial plane", fig.pos="H", out.extra=""}
plot(pca.output$ind$coord, col=0, las=1, cex.axis=.7, cex.lab=.8, xlab="", ylab="")
grid()
points(pca.output$ind$coord, col="royalblue1", pch=20)
abline(h=0)
abline(v=0)

points(pca.output$quali.sup$coord, col="orangered1", pch=20)
text(pca.output$quali.sup$coord[,1], pca.output$quali.sup$coord[,2] + 0.7, col="orangered1", labels = c("no match", "match"))
```
</center>

The data cloud shown in the figure looks like no more than a blob. At least visually, it is impossible to tell the clusters, if there are any. Probabilistic clustering methods like k-means or EM probably will not work very well. Indeed, in the figure below, it is obvious how, for small numbers, it classifies pretty linearly, but starts losing quality as `k` increases (the Calinski-Harabassz index can be seen in the title). A hierarchical clustering method could be more fitting. However, judging by the ibtained CH indices for the same set of `k`, the results are worse.

<center>
```{r echo=F, fig.cap="\\label{fig:kmeans}Result of k-means with k = [2,6]", fig.height=3}
mar <- par("mar")
par(mfrow=c(1,5), mar=c(1,0,1,0))
for (k in 2:6) {
  kmeans.res <- cclust(pca.output$ind$coord, centers=k, method = "kmeans")
  kmeans.ch <- calinhara(pca.output$ind$coord, kmeans.res$cluster)
  plot(pca.output$ind$coord, col=0, xaxt="n", yaxt="n", xlab="", ylab="", main=paste("k=", k, " (CH=", round(kmeans.ch, 2), ")", sep = ""))
  points(pca.output$ind$coord, col=rainbow(k)[kmeans.res$cluster])
}
par(mfrow=c(1,1), mar=mar)
```
</center>

# Modelling

The goal of this analysis is to find the learning algorithm or predictor that is able to build the best model for binary classification. The results of a predictive binary classification are usually represented in a confusion matrix, from which different quality measurements can be obtained. The ones we are most interested in are the recall and the selectivity, which measure the proportion of positively labelled correct predictions and negatively labelled correct predictions, respectively. The mean of these, called the balanced accuracy, can be used to obtain a more generalist metric.

The holdout method will be used for **validation**. This consists on spliting the dataset into a training set and a test set. In our case, following the standard criterion, we built the training set with two thirds of the observations, chosen randomly. The remaining third comprises the test set. The training set has been used for the modelling, using five-fold and ten-fold cross validation (depending on the type of model and how time consuming the building process can be) to build many different models with the same parameters, and choosing the one with the most accuracy.

```{r include=F}
N <- nrow(speed.dating)

learn <- sample(1:N, round(2 * N / 3))

nlearn <- length(learn)
ntest <- N - nlearn

speed.dating.cont <- data.frame(scale(speed.dating[,sapply(speed.dating, is.numeric)], scale=F), match=speed.dating$match)
speed.dating.cat <- speed.dating[,sapply(speed.dating, is.factor)]
```

Next, the results and conclusions for the four different predictors that we tried. These are the MLP articial neural network, random forests, \textcolor{red}{and so on and so forth}.

## MLP Artificial Neural Networks

```{r echo=F}
nnet.train.k <- readRDS("nnet-train-k-results")
plot(nnet.train.k$results[,3], col=0, xlim=c(3,16), las=1, xaxt="n")
axis(1, at=3:16)
grid()
for (decay in seq(from=0.1, to=0.5, by=0.1)) {
  points(nnet.train.k$results[nnet.train.k$results[,2] == decay, c(1,3)], col=brewer.pal(5, "Accent")[decay * 10], type="o", pch=20)
}
legend("bottomleft", legend=paste("decay =", seq(from=0.1, to=0.5, by=0.1)), col=brewer.pal(5, "Accent"), lty=1)
```

```{r echo=F}
prediction <- predict(nnet.train.k$finalModel, newdata = speed.dating.cont[-learn,], type = "class")
knitr::kable(as.matrix(confusionMatrix(table(prediction, speed.dating.cont$match[-learn]))))
```