---
title: "Speed Dating"
author: "Mehmet Fatih Cagil & Unai Perez"
header-includes:
  - \usepackage{xcolor}
  - \usepackage{float}
  - \usepackage{wrapfig}
  - \usepackage{booktabs}
  - \renewcommand{\baselinestretch}{1.15}
output:
  pdf_document:
    keep_tex: no
---

```{r include=F}
set.seed(666)
library(stringdist)
library(mice)
library(chemometrics)
library(nnet)
library(caret)
library(FactoMineR)
library(shape)
library(e1071)
library(fpc)
library(cclust)
library(RColorBrewer)
library(randomForest)

norm.score <- function(row) {
  m <- max(row, na.rm = T)
  s <- sum(row, na.rm=T)
  lapply(row, function(x) 10 * ifelse(is.na(x), 0, x) / (m * (1 + (100 - s) / 100)))
}

speed.dating <- read.csv("speeddating.csv", na.strings = c('', '?'))
```

# Introduction

The _Speed Dating_ dataset is the result of a experiment run by Fisman et al. from 2002 to 2004 The experiment consisted in engaging participant subjects, drawn from students in graduate and professional schools at Columbia University. These subjects would share conversations for four minutes and then they would decide if they both accepted in meeting each other again. Before the dating event, participants would fill in a form giving themselves a rating for **attractiveness**, **sincerity**, **intelligence**, ability to be **funny** and **ambition**. They also had to rate their interests in different topics. After the dating event, the participants would need to rate their partners' on the same characteristics for which they rated themselves. The dating event was organized in waves, and the participants randomly distributed among them, so that subjects in one wave would not interact with participants in other waves.

Below, a detailed explanation of the variables. The names of the variables are not too self-explanatory. Whenever the notation "`d_`" is used, it refers to a categorically discretized version of a numerical variable that is also in the set. Whenever "`_o`" is used, it has the same meaning as without it, but for subject 2 instead of 1.

- `has_null`: Binary feature that is `1` if there is any missing value in the row.
- `wave`: The number of the wave in which the pairing took place.
- `gender`: Categorical feature specifying the gender of subject 1.
- `age[_o]`: Age of subjects 1 and 2, respectively.
- `[d_]d_age`: Features that represent the age difference of the subjects.
- `race[_o]`: Categorical features expressing the race of subjects 1 and 2, respectively.
- `samerace`: Binary feature that is `1` if both subjects are of the same race.
- `[d_]importance_same_{race|religion}`: A score from 0 to 10 expressing how important it is that the other subject shares race and religion, respectively.
- `field`: Categorical variable describing the field of the studies of the subjects.
- `[d_]pref_o_{attractive|sincere|intelligence|funny|ambitious|shared_interests}`: A score from 0 to 100 representing the preference of the subject 2 regarding each one of the characteristics mentioned earlier in their partner.
- `[d_]{attractive|sincere|intelligence|funny|ambitious|shared_interests}_o`: A score from 0 to 10 representing the score subject 2 gives to subject 1 for each of the characteristics.
- `[d_]{attractive|sincere|intelligence|funny|ambitious|shared_interests}_important`: A score from 0 to 100 representing how important is for the subject 1 each of the characteristics in their partner.
- `[d_]{sports|tvsports|exercise|dining|museums|art|hiking|gaming|clubbing|reading|tv |theater|movies|concerts|music|shopping|yoga}`: A score from 0 to 10 representing the interest the subject 1 has in each of the topics.
- `[d_]interests_correlate`: The correlation, measured in the range [-1,1], of the interests of the two subjects.
- `[d_]expected_happy_with_sd_people`: A score from 0 to 10 of how satisfied the subject 1 expects to be with the people met in the speed dating.
- `[d_]expected_num_interested_in_me`: Out of the 20 people subject 1 will meet, the amount they expect to be interested in them after the date.
- `[d_]expected_num_matches`: The amount of matches the subject 1 expects to get.
- `[d_]like`: A binary feature that is `1` if the subject 1 liked its date.
- `[d_]guess_prob_liked`: A probability, guessed by subject 1, of whether they think the subject 2 liked the date.
- `met`: A binary feature that is `1` if the subject 1 new subject 2 beforehand.
- `decision[_o]`: Binary features that are `1` if the subjects decide they want to see the other subject in the future.
- `match`: Target binary feature. `1` if both subjects agree on seeing each other in the future.

The features that are scored out of 100 work differently in comparison to regular grades. The subjects were given a 100 points to distribute among the five criteria and the `shared_interests` criteria. This means that the sum of the scores in these blocks should add up to 100.

# Data preprocessing

We know data pre-processing is long and tough (let's not make any joke about this), and this dataset was not easy to work with from the get-go. Here is how we preprocessed the data.

## Feature name typos

One weird characteristic about this dataset is the lack of consistency and the bad ortography in the names of the features. In the list above, all the names have been correctly written. However, "sincere" is many times misspelled as "sinsere", or "ambitious" as "ambitous". Also, sometimes "ambition" is used instead. The first step is to just fix the names of the variables.

```{r include=F}
names(speed.dating)[which(names(speed.dating) %in% c("sinsere_o", "ambitous_o", "d_sinsere_o", "d_ambitous_o", "ambtition_important", "d_ambtition_important"))] <- 
  c("sincere_o", "ambitious_o", "d_sincere_o", "d_ambitious_o", "ambition_important", "d_ambition_important")
```

## Missing values

A first look at the dataset shows that missing data is encoded as question marks (`?`). These can be transformed into R's `NA` as soon as they are read, and can be dealt with later on. In total, `r length(which(sapply(speed.dating, anyNA)))` features have at least one missing value. If feature `has_null` is correctly labelled, then the amount of observations with `has_null` set to 1 (which equals to `r length(which(speed.dating$has_null == 1))`) and the amount of observations with a `NA` in any of the columns (which equals to `r nrow(speed.dating) - length(which(complete.cases(speed.dating)))`) should be, and are, the same. A huge majority (`r round(100 * nrow(speed.dating[speed.dating$has_null == 1,]) / nrow(speed.dating), 2)`%) of the observations in the dataset have some missing value, just deleting them is not an option.

The summary reveals a series of potentially meaningful patterns when it comes to missing values. Features can be grouped as shown in the list of the previous section, into what would translate as (let's call it) a block of questions from the form. To begin with, all of the features from the block of interests (like `sports` or `hiking`) have the same amount of missing values, `r length(which(is.na(speed.dating$sports)))`, all of them for the same individuals. The block that represents the factors of the partner that are most important for the subject, like `attractive_important`, show a similar behaviour. That is also the case for the self ratings in the five criteria, like `attractive` and `intelligence`. They have at least the same amount of missing values, and most of do coincide in the same observations. It seems like a lot of people refused to fill the form before the dates! These people will be of no use for our research, so those rows will be deleted.

```{r include=F}
speed.dating <- speed.dating[!(is.na(speed.dating$sports)),]
```

Looking at the summary after the deletion of said rows, two more patterns can be seen. Self ratings, like `intelligence`, seem to be unknown more the same group of observations, exactly `r nrow(speed.dating[is.na(speed.dating$intelligence),])`. Meanwhile, the partner's preference features, like `pref_o_sincere`, share a very similar amount of missing values. Although the missing values of these two groups do not coincide in the same observations, it is interesting to see that specific subgroups of variables have missing values for the same individuals. These two groups are removed.

```{r include=F}
speed.dating <- speed.dating[!(is.na(speed.dating$pref_o_sincere)),]
speed.dating <- speed.dating[!(is.na(speed.dating$attractive)),]
```

There seems to be a couple more of patterns. Both age related features are missing in the same observations, but the `d_age` feature describing age difference is there, so we could consider that data is not missing. Oddly enough, these missing observations belong to couples with more than 18 years of difference in age. The last pattern seems to relate the missing values of the ratings given by the partners (`attractive_o` and so on) and the ratings given by the subjects to the partner (`attractive_partner` and so on). But there is no actual connection.

After those patterns, all of the participants that did not respond at all to, at least, one of the blocks will be deleted. This is only the case for two blocks. The first one is the rating that subject 2 gives subject 1 regarding the five criteria. The second block is the opposite, the ratings subject 1 give to subject 2 regarding those criteria. The features that represent categorically the same meaning (those named the same but starting with "`d_`") do not seem to hold any value for these individuals anyway; they are all assigned, seemingly by default, to the category `[0-5]`. All these observations are also deleted.

```{r include=F}
speed.dating <- speed.dating[-which(apply(is.na(speed.dating[, 28:33]), 1, all)),]
speed.dating <- speed.dating[-which(apply(is.na(speed.dating[, 62:67]), 1, all)),]
```

Finally, we are going to check those features that represent scores out of 10, this is, those blocks made up of the variables named `_partner` and `_o`. Some of these have many missing values. The one with the most NAs is `shared_interests_partner`, which has `r length(which(is.na(speed.dating$shared_interests_partner)))`. If we look at the values of the equivalent categorical features, all of these NA are assigned to the group `[0-5]`. The missing value could be caused by the subject just not caring too much about that one criterion in particular. So, we have decided to assign `0` to these missing values. The same thing can be applied to the missing values of the features with scores out of 100 (`pref_o` and `_important` blocks).

```{r include=F}
for (i in c(16:21, 28:33, 40:45, 62:67)) {
  speed.dating[is.na(speed.dating[,i]), i] <- 0
}
```

## Variable typing and coherence

The summary shows that some grades are over the range. For example, one of the participants graded herself as an 11 out of 10 in `funny`, which is funny by itself because the subject is studying the _law_ field, and we don't know any funny lawyer. For feature `gaming`, there are 5 different participants that graded their interests as 14 out of 10. These definitely need to be fixed. There are 38 variables that should range from 0 to 10, from which 4 have some value out of those bounds. For the `gaming` example, categorical feature `d_gaming` groups the incoherent values into `[9-10]`. This makes us believe that all grades over 10 should be turned back into just 10.

```{r include=F}
speed.dating$gaming[!(is.na(speed.dating$gaming)) & speed.dating$gaming > 10] <- 10
speed.dating$funny_o[!(is.na(speed.dating$funny_o)) & speed.dating$funny_o > 10] <- 10
speed.dating$attractive_o[!(is.na(speed.dating$attractive_o)) & speed.dating$attractive_o > 10] <- 10
speed.dating$reading[!(is.na(speed.dating$reading)) & speed.dating$reading > 10] <- 10
```

As explained in the introduction, there is a block of features that represent the preferences of the subjects by distributing 100 points among them. This is not always the case, though. Some participants did not actually completely use the 100 points, but this is addressed later in the document. The problem comes from those that used more than 100 points. These guys clearly do not know how to add up. For the sake of consistency, we have decided to remove these observations. Before, we have checked their field of studies just out of curiosity, and many of these are mathematicians and engineers. Future is bright!

```{r include=F}
speed.dating <- speed.dating[-c(which(floor(apply(speed.dating[,16:21], 1, sum)) > 100), which(floor(apply(speed.dating[,40:45], 1, sum)) > 100)),]
```

Some of those subjects, though, just exceed the 100 points by decimals. To avoid having problems in the future, these decimals have been substracted from them so that their score will add up to exactly 100. 

```{r include=F}
speed.dating[which(apply(speed.dating[,16:21], 1, sum) > 100),16:21] <- speed.dating[which(apply(speed.dating[,16:21], 1, sum) > 100),16:21] - ((apply(speed.dating[which(apply(speed.dating[,16:21], 1, sum) > 100),16:21], 1, sum) - 100) / 6)
speed.dating[which(apply(speed.dating[,40:45], 1, sum) > 100),40:45] <- speed.dating[which(apply(speed.dating[,40:45], 1, sum) > 100),40:45] - ((apply(speed.dating[which(apply(speed.dating[,40:45], 1, sum) > 100),40:45], 1, sum) - 100) / 6)
```

Finally, seeing that some of the types automatically assigned to the features by R when reading the file are wrong, we decided to change them. Also, we have discussed whether the score features should be considered factors but, due to one process run for feature extraction, numbers with decimals are obtained. Some come already with decimals from the file, too. We have decided that it would obviously cause innaccuracy, so scores will be considered as numeric values. We already have categorical values out of the box for scores anyway.

```{r include=F}
speed.dating$has_null <- as.factor(speed.dating$has_null)
speed.dating$wave <- as.factor(speed.dating$wave)
speed.dating$samerace <- as.factor(speed.dating$samerace)
speed.dating$decision <- as.factor(speed.dating$decision)
speed.dating$decision_o <- as.factor(speed.dating$decision_o)
speed.dating$match <- as.factor(speed.dating$match)
```

## Feature selection and extraction

The block of features representing the preferences of the subjects regarding the five criteria (`_important` block for subject 1 and `pref_o_` block for subject 2) are scores out of 100. The rest of scores are out of 10. But, as explained in the introduction, these features work differently. If the out of 100 scores were regular scores, it would be better to multiply by 10 the out of 10 scores to avoid inaccuracies. In this case, we have decided to use the formula below to turn each feature in its proportional score between 0 and 10, based on the criteria with the highest score, with a penalization in the denominator for those that did not distribute the 100 points.

$$ score^{(i)}_{10} = \frac{10 \times score^{(i)}_{100}}{\max(score_{100}) \times (1 + \frac{100 - \sum_{i'} score^{(i')}_{100}}{100})} $$

```{r include=F}
for (i in 1:nrow(speed.dating)) {
  speed.dating[i,16:21] <- norm.score(speed.dating[i, 16:21])
  speed.dating[i,40:45] <- norm.score(speed.dating[i, 40:45])
}
```

This transformation would need to be stored in completely different variables, but we have decided to do without the original out of 100 scores. Thus, they have been overwritten. That has been the case too for the equivalent categorical variables (the ones with same name but starting with `d_`), which have been replaced using the same categories as the rest of the categorical variables: `[0-5]`, `[6-8]` and `[9-10]`. We think these represent the different feelings a person can have towards something appropriately; they could be translated as "dislike", "like", and "love".

```{r include=F}
for (i in c(16:21, 40:45)) {
  speed.dating[,i+6] <- cut(speed.dating[,i], breaks=c(0,5,8,10), include.lowest = T, labels=c("[0-5]","[6-8]","[9-10]"))
}
```

Feature `field` brings interesting information to the table. With this information, we could find correlation between the field of study of the subjects and, maybe, the probability of this being influential in the final choice. Nevertheless, the actual categories of the feature are filled with typos, differently formatted but equivalent answers, and globally or specifically ambiguous answers. For example, `finance` and `finanace` are two different categories, as well as `finance/economics` and `finance&economics`, and, finally, there are categories just labelled as `money` or the different categories `english` and `english education`, whose differences we are not really able to tell.

We tried performing some sort of entity resolution by transforming categories to lower case, then applying Levenshtein edit distance to find similarities. A big maximum distance (like 5) would match `english` from `polish`, an unwanted result. A small maximum distance (like 2) would detect most typos but leave out categories with larger differences. It all came back to manual selection, which for `r length(levels(speed.dating$field))` different categories is cumbersome and tiring. We decided to get rid of the feature completely.

```{r include=F}
speed.dating$field <- NULL
```

Now that we are discarding features, we decide to discard the features `age`, `age_o` and `d_age`. We do not think the actual ages are all that important, or event the age difference per se; variable `d_d_age`, categorical representation of the age difference, will do just fine. Also, feature `expected_num_interested_in_me` will be discarded, due to its `r length(which(is.na(speed.dating$expected_num_interested_in_me)))` missing variables. Features `importance_same_religion` and `d_importance_same_religion` get discarded too because we do not actually have the religion data, so there is nothing to compare it to (They must have slipped up!). And we do not need `has_null` anymore. The rest of features, a total of `r ncol(speed.dating) - 10`, are staying. We think that having different representations of the same data directly from the raw file saves us time and effort, and opens up the possibility to apply different models without more extracted features.

```{r include=F}
speed.dating$age <- NULL
speed.dating$age_o <- NULL
speed.dating$d_age <- NULL
speed.dating$expected_num_interested_in_me <- NULL
speed.dating$met <- NULL
speed.dating$decision <- NULL
speed.dating$decision_o <- NULL
speed.dating$importance_same_religion <- NULL
speed.dating$d_importance_same_religion <- NULL
speed.dating$has_null <- NULL
```

## Data imputation

After the all the preprocessing so far, only `r length(which(sapply(speed.dating, anyNA)))` features with NAs are left. These have been imputed using the MICE imputation method, using the five criteria parameters and the `_important` block as the only supplementary variables. Using the whole dataset slows down the whole process and makes it last up to 30 times more. Using only these features still gives realistic imputation, so we considered it appropriate.

```{r include=F}
mice.speed.dating <- mice(speed.dating[,c(which(sapply(speed.dating, anyNA)), 37:42, 47:52)], method = 'pmm', seed = 500)
mice.speed.dating.complete <- complete(mice.speed.dating)
speed.dating[, which(sapply(speed.dating, anyNA))] <- mice.speed.dating.complete[,1:length(which(sapply(speed.dating, anyNA)))]
```

## Outlier detection

The dataset has `r ncol(speed.dating)` left after the preprocessing. They are still plenty, so trying to find univariate outliers is not only annoying, but also useless. For the multivariate outlier detection, the Mahalanobis distance has been used. Figure \ref{fig:multivar-outliers} shows a graphical representation of the distances of each observation, where the red dots represent the observations with a match, and the blue ones observations without a match. On plain sight, there does not seem to be any correlation between the outcome and the distance (or the _outlierness_, we could say). If anything, we can state that the few that stand out the most are non-matches. The 2.5% of the observations with the highest Mahalanobis distance, shown in the figure \ref{fig:multivar-outliers} above the line, will be considered outliers and removed.

```{r multivar-outliers, echo=F, fig.cap="Graphical representation of the Mahalanobis distances for each observation", fig.align="center", fig.height=3.5, fig.width=5}
dating.outliers <- Moutlier(scale(speed.dating[,sapply(speed.dating, is.numeric)]), plot=F, quantile = 0.975)
par(mfrow=c(1,2))
plot(dating.outliers$md, main="Mahalanobis Distance", ylab="Distance", xlab="", col=ifelse(speed.dating$match == "1", "orangered1", "royalblue1"), las=1, cex.axis=.75, pch=20, ylim=c(0, max(dating.outliers$rd)), xaxt="n", cex.main=.8)
plot(dating.outliers$rd, main="Robustified Mahalanobis", ylab="", xlab="", col=ifelse(speed.dating$match == "1", "orangered1", "royalblue1"), las=1, cex.axis=.75, pch=20, ylim=c(0, max(dating.outliers$rd)), xaxt="n", cex.main=.8)
cutoff <- sort(dating.outliers$rd, decreasing = T)[floor(nrow(speed.dating) * 0.025)]
abline(h = cutoff, lty=2, col="red4", lwd=2.25)
par(mfrow=c(1,1))
```

```{r include=F}
speed.dating <- speed.dating[-order(dating.outliers$rd, decreasing = T)[1:floor(nrow(speed.dating) * 0.025)],]
```

```{r include=F}
speed.dating.pca <- speed.dating[,c(which(sapply(speed.dating, is.numeric)), 112)]
for (i in 1:52) {
  speed.dating.pca[,i] <- speed.dating.pca[,i] - mean(speed.dating.pca[,i])
}
pca.output <- PCA(speed.dating.pca, quali.sup = c(53), graph = F)
pca.best.repr.vars <- order(sqrt(pca.output$var$coord[,1]^2 + pca.output$var$coord[,2] ^ 2), decreasing = T)[1:10]
```

```{r pca-ind-repr, echo=F, fig.cap="Representation of individuals in first factorial plane", fig.height=3.25, fig.width=4, fig.pos="!b", out.extra="", fig.align="center"}
plot(pca.output$ind$coord, col=0, las=1, cex.axis=.7, cex.lab=.8, xlab="", ylab="")
grid()
points(pca.output$ind$coord, col="royalblue1", pch=20)
abline(h=0)
abline(v=0)

points(pca.output$quali.sup$coord, col="orangered1", pch=20)
text(pca.output$quali.sup$coord[,1], pca.output$quali.sup$coord[,2] - 1.2, col="orangered1", labels = c("no match", "match"), cex=.7)
```

## Data visualization

PCA allows us to see a representation of the dataset in two dimension of the data. With this, we will perform a few clustering methods and see if we can extract something useful out of it. PCA can only be used with numeric explanatory variables, so the variable `match` was added as qualitative suplementary variable. This means a total of `r length(which(sapply(speed.dating, is.numeric))) + 1` variables. Figure \ref{fig:pca-var-repr} shows the representation in the first factorial plane of _only_ the 10 variables with the most representation and, still, they are not really well represented, probably due to the high dimensionality of the data. In figure \ref{fig:pca-ind-repr} the representation of the individuals and the categories of the `match` supplementary variable can be seen. None of those seems to be too tied to any of the dimensions.

```{r pca-var-repr, echo=F, fig.height=3.75, fig.cap="Variable representation in the first factorial plane", fig.pos="H", out.extra="", fig.align="center"}
plot(pca.output$var$coord, col=0, xlim=c(-2.5,1.5), ylim=c(-.8,.8), las=1, cex.axis=.7, cex.lab=.8, xlab="", ylab="")
abline(h=0, col="gray")
abline(v=0, col="gray")
plotellipse(1, 1)
invisible(Arrows(0, 0, pca.output$var$coord[pca.best.repr.vars,1], pca.output$var$coord[pca.best.repr.vars,2], arr.type = "simple", col=rainbow(10)))
legend("topleft", legend=row.names(pca.output$var$coord[pca.best.repr.vars,]), col=rainbow(10), lty=1, cex=.7, bg="white")
```

The data cloud shown in figure \ref{fig:pca-ind-repr} looks like no more than a blob. At least visually, it is impossible to tell the clusters, if there are any. Probabilistic clustering methods like k-means or EM probably will not work very well. Indeed, in figure \ref{fig:kmeans}, it is obvious how, for small numbers, it classifies pretty linearly, but starts losing quality as `k` increases (the Calinski-Harabassz index can be seen in the title). A hierarchical clustering method could be more fitting. However, judging by the ibtained CH indices for the same set of `k`, the results are worse.

# Modelling

The goal of this analysis is to find the learning algorithm or predictor that is able to build the best model for binary classification. The results of a predictive binary classification are usually represented in a confusion matrix, from which different quality measurements can be obtained. The ones we are most interested in are the positive and the negative predictive values (PPV and NPV), which measure the proportion of positively labelled correct predictions and negatively labelled correct predictions, respectively. The mean of these, called the balanced accuracy, can be used to obtain a more generalist metric.

```{r kmeans, echo=F, fig.height=2, fig.cap="Result of k-means with k = [2,6]", fig.pos="!t", out.extra=""}
mar <- par("mar")
par(mfrow=c(1,5), mar=c(1,0,1,0))
for (k in 2:6) {
  kmeans.res <- cclust(pca.output$ind$coord, centers=k, method = "kmeans")
  kmeans.ch <- calinhara(pca.output$ind$coord, kmeans.res$cluster)
  plot(pca.output$ind$coord, col=0, xaxt="n", yaxt="n", xlab="", ylab="", main=paste("k=", k, " (CH=", round(kmeans.ch, 2), ")", sep = ""))
  points(pca.output$ind$coord, col=rainbow(k)[kmeans.res$cluster])
}
par(mfrow=c(1,1), mar=mar)
```

The holdout method will be used for **validation**. This consists on spliting the dataset into a training set and a test set. In our case, following the standard criterion, we built the training set with two thirds of the observations, chosen randomly. The remaining third comprises the test set. The training set has been used for the modelling, using five-fold and ten-fold cross validation (depending on the type of model and how time consuming the building process can be) to build many different models with the same parameters, and choosing the one with the most accuracy.

```{r include=F}
N <- nrow(speed.dating)

learn <- sample(1:N, round(2 * N / 3))

nlearn <- length(learn)
ntest <- N - nlearn

speed.dating.cont <- data.frame(scale(speed.dating[,sapply(speed.dating, is.numeric)], scale=F), match=speed.dating$match)
speed.dating.cat <- speed.dating[,sapply(speed.dating, is.factor)]
```

Next, the results and conclusions for the four different predictors that we tried. These are the MLP articial neural network, random forests, \textcolor{red}{and so on and so forth}.

## MLP Artificial Neural Networks

The MultiLayer Perceptron neural network has been tried with several different decays and sizes, and it has been built only with the continuous variables of the dataset. However, we have manually tuned the maximum amount of iterations allowed for back-propagation by visually analyzing the text output. First, with `maxit` set to 150, it was obvious that when the maximum was reached the weights still had room for improvement. We have settled in with 400. Even though the cost still was getting reduced iteration by iteration, the difference stopped being so significant after around 300 iterations.

```{r ann-confusion-matrix, echo=F}
nnet.train.k <- readRDS("nnet-train-k-results")
prediction <- predict(nnet.train.k$finalModel, newdata = speed.dating.cont[-learn,], type = "class")
conf.matrix.ann <- confusionMatrix(table(prediction, speed.dating.cont$match[-learn]))
knitr::kable(as.matrix(conf.matrix.ann), caption = "Confusion matrix for MLP", format="latex", valign="H", booktabs=T)
```

As for the decay and sizes of the hidden layer, we have tried all possible combinatios of the ranges \{`r paste(seq(from=0.1, to=0.5, by=0.1), collapse = ",")`\} for the decay and [3,16] for the size. The result is the one shown in figure \ref{fig:ann-accuracy-plot}: The configuration that obtains the best accuracy, by a small gap, is a 4 neuron-sized hidden layer with a decay of 0.2. If anything, it is clear that adding more neurons to the hidden layer makes the end result worse, at least in this case. Also, size 4 is the best option, no matter the decay. Finally, for the chosen model, the confusion matrix over the test set can be seen on table \ref{tab:ann-confusion-matrix}. The PPV, `r round(conf.matrix.ann$byClass[3], 3)`, is pretty good, but the NPV is `r round(conf.matrix.ann$byClass[4], 3)` is too low, closing in to only 50%.

```{r ann-accuracy-plot, echo=F, fig.cap="Plot of the accuracy over the different configurations", fig.pos="th", out.extra="", fig.height=4}
plot(nnet.train.k$results[,3], col=0, xlim=c(3,16), las=1, xaxt="n", xlab="", ylab="Accuracy", cex.axis=.75)
axis(1, at=3:16, cex.axis=.75)
grid()
for (decay in seq(from=0.1, to=0.5, by=0.1)) {
  points(nnet.train.k$results[nnet.train.k$results[,2] == decay, c(1,3)], col=brewer.pal(5, "Accent")[decay * 10], type="o", pch=20)
}
legend("bottomleft", legend=paste("decay =", seq(from=0.1, to=0.5, by=0.1)), col=brewer.pal(5, "Accent"), lty=1, cex=.75)
```

## Random forests

To build the random forest models, the two types of features (categorical ones from one side and continuous ones from the other) have been fed to it separately. The forest built with the continuous features gave slightly better preliminary results, so we have worked with it. To find the best possible model, we have parametrized the treen spawn (`ntree`), leaving the variable sample size static to \(\lfloor\sqrt{D} \times 3\rfloor =\) `r floor(sqrt(ncol(speed.dating.cat)) * 3)` (\(D\) being the amount of dimensions), and built it with 10-fold cross-validation. The accuracy did not vary too much among these models, but the one with the highest accuracy usually ends up using 175 trees.

```{r rf-var-importance, echo=F, out.extra="", fig.cap="Barplot of the importance of variables in the forest", fig.align="center", fig.pos="H", fig.height=4, fig.width=5.25}
best.rf.model <- readRDS("best-rf-model")
accuracies <- readRDS("rf-accuracies")
best.rf.model.2 <- readRDS("best-rf-model2")
accuracies.2 <- readRDS("rf-accuracies2")

rf.imp.barplot.2 <- barplot(sort(best.rf.model.2$finalModel$importance, decreasing = T)[1:10], col=rev(brewer.pal(10, "RdYlGn")))
text(rf.imp.barplot.2, y = 2, srt=90, labels=names(speed.dating.cont[,-51])[order(best.rf.model.2$finalModel$importance, decreasing = T)][1:10], xpd=F, adj = 0, cex=.9)
```

```{r include=F}
prediction <- predict(best.rf.model.2$finalModel, speed.dating.cont[-learn,-51], type="class")
conf.matrix.rf <- confusionMatrix(table(prediction, speed.dating.cont$match[-learn]))
```

When plotting the importance of the each variables, we found out that the feature with the highest importance (by a significant gap) was `like`, which expresses whether the subject liked their date. In the dataset, there is only one observation in which the subject liked the date but there was no match. So, we decided to try to build another random forest without the bias of this feature, and the result can be seen in figure \ref{fig:rf-var-importance}. We think no one will be shocked by how attractiveness and shared interests play such an important role in deciding the outcome of a date. But the most important characteristic seems to be whether the partner is funny or not. Now, that is more of a surprise. Table \ref{tab:confusion-matrix-rf} shows the confusion matrix obtained by the prediction, which scores a PPV of `r round(conf.matrix.rf$byClass[3], 3)` and a NPV of `r round(conf.matrix.rf$byClass[4], 3)`, which tells us obviously that this is a very reliable model, the most so far.

```{r confusion-matrix-rf, echo=F}
prediction <- predict(best.rf.model.2$finalModel, speed.dating.cont[-learn,-51], type="class")
knitr::kable(table(prediction, speed.dating.cont$match[-learn]), format="latex", caption = "Confusion matrix for the random forest", booktabs=T)
```

## SVM

```{r include=F}
svm.model.linear <- readRDS("svm-model-linear")
svm.model <- readRDS("svm-model")
svm.model2 <- readRDS("svm-model2")
```

The support vector machine has been tried with different kernels and parameters, and it has been built only with the continuous variables of the dataset. We decided to use the radial kernel in the model, since its training error is slightly less than the other options. `r round(1 - max(svm.model.linear$results[,2]), 3)` is the empirical error rate of the linear kernel, while radial kernel has an empirical error rate of `r round(1 - max(svm.model$results[,3]), 3)`. 

```{r svm-param-tuning, echo=F, out.extra="", fig.height=4, fig.width=5, fig.cap="Plot of the parameter tuning for SVM", fig.align="center", fig.pos="H"}
plot(svm.model2$results[,3], col=0, ylim=c(.83,.85), xlim=c(3.3,5), xlab="C", ylab="Accuracy", las=1, cex.axis = .9, cex.lab=.8)
grid()
for (sigma in unique(svm.model2$results[,1])) {
  points(x = svm.model2$results[svm.model2$results[,1] == sigma, 2], type="o", y = svm.model2$results[svm.model2$results[,1] == sigma, 3], pch=20, col=rainbow(4)[sigma / 0.005])
}
legend("bottomleft", legend = paste("Sigma =", unique(svm.model2$results[,1])), col=rainbow(4), lty=1, cex=.8, bg="white")
points(svm.model2$results[order(svm.model2$results[,3], decreasing = T)[1],2:3], col="red")
```

```{r include=F}
prediction <- predict(svm.model, speed.dating.cont[-learn,])
conf.matrix.svm <- confusionMatrix(table(prediction, speed.dating.cont$match[-learn]))
```

During the modelling, via 5-fold cross validation method, we found out that the optimal values for the parameters are \(C=4\) and the \(\sigma=0.01\). We decided to make some fine tuning of these parameters, again with 5-fold cross validation method. The range of values \{0.005,0.01,0.015,0.02\} has been used for the different \(\sigma\), and the values \{3.3,3.5,3.8,4,4.2,4.5\} as the possible costs. After the training, the best found model did not change \(\sigma\) from 0.01 and shifted cost (just a bit) to 4.2. This is represented in figure \ref{fig:svm-param-tuning}. But the new lowest empirical error is `r round(1 - max(svm.model2$results[,3]), 3)`, which, for this case, is worse than the original. Therefore, we stay with the original, non-tuned model. The confusion matrix of this model over the test set can be seen in table \ref{tab:conf-matrix-svm}: The PPV is `r round(conf.matrix.svm$byClass[3], 3)` and the NPV is `r round(conf.matrix.svm$byClass[4], 3)`.

```{r conf-matrix-svm, echo=F, fig.align="center", out.extra=""}
knitr::kable(as.matrix(conf.matrix.svm), format="latex", booktabs=T, caption="Confusion matrix of the SVM model")
```

## KNN

```{r include=F}
knn.fit <- readRDS("knn-model")
```

The k nearset neighbors method has been tried with 10-fold cross validation method and we found out that optimal value for number of neighbors is \(k=17\). Change in accuracy with respect to number of neighbors can be examined in the figure \ref{tab:knn-lot}  


```{r knn-plot, echo=F, out.extra="", fig.height=4, fig.width=5, fig.cap="Plot of KNN training", fig.align="center", fig.pos="H"}
plot(knn.fit, ylab = "Accuracy", xlab = "# Neighbors")
#plot(knn.fit$results[,2])
```

```{r include=F}
knn.prediction <- predict(knn.fit, newdata = speed.dating.cont[-learn,])
conf.matrix.knn <- confusionMatrix(table(knn.prediction, speed.dating.cont$match[-learn]))
```

The confusion matrix of this model over the test set can be seen in table \ref{tab:conf-matrix-knn}: The PPV is `r round(conf.matrix.knn$byClass[3], 3)` and the NPV is `r round(conf.matrix.knn$byClass[4], 3)`.

```{r conf-matrix-knn, echo=F, fig.align="center", out.extra=""}
knitr::kable(as.matrix(conf.matrix.knn), format="latex", booktabs=T, caption="Confusion matrix of the KNN model")
```

## Logistic regression

```{r include=F}
glm.model <- glm(match ~ ., data = speed.dating.cont[learn,], family = binomial(link=logit))
prediction <- predict(glm.model, newdata = speed.dating.cont[-learn,], type="response")
conf.matrix.glm <- confusionMatrix(table(prediction > 0.5, speed.dating.cont$match[-learn] == 1))
```

This type of generalized linear model is a typical choice for binary classification. However, cross-validation can not be used because it is deterministic, and would not give out different models after each run, so we have proceeded the regular way. With the logistic regression, we have calculated the probability of an instance to have 1 in the `match` target variable. The model built using all the continuous features results in the confusion matrix, generated with the test set, shown in table \ref{tab:conf-matrix-glm}, and the results are reasonably good. The accuracy (`r round(conf.matrix.glm$overall[1], 3)`) and PPV (`r round(conf.matrix.glm$byClass[3], 3)`) are significantly good for such a simple model and, compared to the other models so far, the NPV (`r round(conf.matrix.glm$byClass[4], 3)`) is also slightly better than the MLP.

```{r conf-matrix-glm, echo=F}
knitr::kable(as.matrix(conf.matrix.glm), booktabs=T, format="latex", caption="Confusion matrix for the logistic regression", valign="H")
```

# Playing around with models

For this section, we have simulated a date and collected real data from two people. With this information, we have created two new rows corresponding to a date between them in which each one of these people fulfilled the part of both subjects in the date. And so, we have used this new instances to put the created models into test. As both rows correspond to the same couple, the predictions for both should be the same.

```{r include=F}
my.data <- data.frame(NA, 10, 20, 20, 20, 20, 10, 7, 7, 9, 9, 5, 7, 25, 15, 20, 15, 15, 10, 6, 8, 8, 8, 7, 7, 8, 6.5, 7, 7, 6, 9, 5, 7, 9, 0, 0, 10, 7, 2, 7, 1, 0, 10, 6, 7, 3, 0, NA, NA, NA, NA, NA, NA)
my.data.2 <- data.frame(NA, 25, 15, 20, 15, 15, 10, 7, 8, 6.5, 7, 7, 6, 10, 20, 20, 20, 20, 10, 6, 8, 6, 5, 8, 7, 7, 9, 9, 5, 7, 7, 0, 8, 8, 7, 8, 8, 3, 5, 3, 0, 5, 8, 8, 9, 3, 0, NA, NA, NA, NA, NA, NA)
names(my.data) <- names(speed.dating.cont)
names(my.data.2) <- names(speed.dating.cont)
new.data <- rbind(my.data, my.data.2)

mice.imp.new.data <- complete(mice(rbind(speed.dating.cont, new.data), method = 'pmm'))
new.data[, c(1, 48:52)] <- mice.imp.new.data[1:2, c(1, 48:52)]

new.data[, "Neural network"] <- predict(nnet.train.k$finalModel, new.data[,-53], type="class")
new.data[,"Random forest"] <- predict(best.rf.model.2$finalModel, new.data[,-53], type="class")
new.data[, "SVM"] <- predict(svm.model, new.data[,-53])
new.data[, "Logistic regression"] <- ifelse(predict(glm.model, new.data[,-53], type="response") > .5, 1, 0)
row.names(new.data) <- c("Row 1", "Row 2")
```

```{r new-data-pred, echo=F}
knitr::kable(new.data[,54:57], format="latex", valign="H", caption="Table of the predictions for the new data", align=rep("r", 3), booktabs=T)
```

And now comes the twist: The data is taken from an actual couple, which means that the prediction should be 1 in all cases. As can be seen in table \ref{tab:new-data-pred}, most of the models return the same outcome for both rows, except for the neural network. Still, only random forest and logistic regression are the only one getting it right. However, with such a small new population, this shall not be considered as any kind of validation error.

# Conclusions

Grosso modo, table \ref{tab:final-results} shows the results gathered throughout the different models. And, so, we know the best result has been obtained by the random forest model.

```{r final-results, echo=F, fig.align="center"}
final.results <- data.frame()
final.results <- rbind(final.results, c(conf.matrix.ann$overall[1], conf.matrix.ann$byClass[3:4]))
final.results <- rbind(final.results, c(conf.matrix.rf$overall[1], conf.matrix.rf$byClass[3:4]))
final.results <- rbind(final.results, c(conf.matrix.svm$overall[1], conf.matrix.svm$byClass[3:4]))
final.results <- rbind(final.results, c(conf.matrix.glm$overall[1], conf.matrix.glm$byClass[3]))
row.names(final.results) <- c("ANN", "Random forest", "SVM", "Logistic Reg.")
colnames(final.results) <- c("Accuracy", "PPV", "NPV")
knitr::kable(final.results, format="latex", booktabs=T, caption="Final comparison of results on validation set")
```